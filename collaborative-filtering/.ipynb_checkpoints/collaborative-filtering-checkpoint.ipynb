{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "The following file summarizes the performed collaborative filtering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbeitspakete:\n",
    "- Filterfunktionen bauen: Random sampling\n",
    "- Evaluation Ã¼ber verschiedene Filterfunktionen & einzelne Algorithmen\n",
    "- RMSE / MAE Plots ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique rows in interactions:  1132367\n",
      "Unique rows in recipes:  231637\n"
     ]
    }
   ],
   "source": [
    "%run functions.py\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from surprise import SVD, NMF, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Load dataset\n",
    "interactions = pd.read_csv('../data/RAW_interactions.csv', sep=',')\n",
    "recipes = pd.read_csv('../data/RAW_recipes.csv', sep=',')\n",
    "print('Unique rows in interactions: ', len(interactions))\n",
    "print('Unique rows in recipes: ', len(recipes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter ideas:\n",
    "- Varianz-basiert ?\n",
    "- Up-/Downsampling ? \n",
    "- Random Sampling (User & Recipes) - sample_size_user / sample_size_recipe in Anzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = interactions[['user_id', 'recipe_id', 'rating']].copy()\n",
    "\n",
    "# Delete all '0' ratings\n",
    "ratings = ratings[ratings.rating != 0]\n",
    "\n",
    "# Filter ratings\n",
    "ratings = filter_user_item(ratings, 1, 1, \"iqr\", \"iqr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 87623\n",
      "\n",
      "Number of users: 77098\n",
      "\n",
      "Number of recipes: 64500\n",
      "\n",
      "Fraction of ratings that is left: 0.08\n",
      "\n",
      "Fraction of users that is left: 0.34\n",
      "\n",
      "Fraction of recipes that is left: 0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of ratings included in the filtered dataset\n",
    "print(f\"Number of ratings that is left: {len(ratings)}\\n\")\n",
    "\n",
    "# Print the number of users included in the filtered dataset\n",
    "print(f\"Number of users that is left: {ratings.user_id.unique().size}\\n\")\n",
    "\n",
    "# Print the number of recipes included in the filtered dataset\n",
    "print(f\"Number of recipes that is left: {ratings.recipe_id.unique().size}\\n\")\n",
    "\n",
    "# Print the fraction of ratings included in the filtered dataset\n",
    "print(f\"Fraction of ratings that is left: {round(len(ratings) / len(interactions), 2)}\\n\")\n",
    "\n",
    "# Print the fraction of users included in the filtered dataset\n",
    "print(f\"Fraction of users that is left: {round(ratings.user_id.unique().size / interactions.user_id.unique().size, 2)}\\n\")\n",
    "\n",
    "# Print the fraction of recipes included in the filtered dataset\n",
    "print(f\"Fraction of recipes that is left: {round(ratings.recipe_id.unique().size / interactions.recipe_id.unique().size, 2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep grid search\n",
    "\n",
    "# Params\n",
    "param_grid = {#\"KNNBasic\" : {\"k\" : [20,40,60],\n",
    "#                             \"sim_options\" : {\"name\" : ['cosine'],\n",
    "#                                              \"user_based\"  : [True, False],\n",
    "#                                             }\n",
    "#                            },\n",
    "#               \"KNNWithMeans\" : {\"k\" : [20,40,60],\n",
    "#                                \"sim_options\" : {\"name\" : ['cosine'],\n",
    "#                                                 \"user_based\"  : [True, False],\n",
    "#                                                }\n",
    "#                                },          \n",
    "    \"SVD\" : {\"n_factors\" : [5, 15, 25, 50]},\n",
    "#               \"NMF\" : {\"n_factors\" : [5, 15, 25, 50]},\n",
    "#               \"SlopeOne\" : {},\n",
    "#               \"Baseline\" : {}\n",
    "             }\n",
    "\n",
    "\n",
    "# Algos\n",
    "algos = {\"SVD\" : SVD, \"NMF\" : NMF, \"SlopeOne\" : SlopeOne, \"Baseline\" : NormalPredictor}\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state = 0) \n",
    "\n",
    "# for each Filter\n",
    "#     for each Algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: Plot performance of best hyperparameter setting per algorithm\n",
    "RMSE / MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "benchmark = []\n",
    "\n",
    "# Cross-validation\n",
    "cv = ShuffleSplit(n_splits = 5, test_size = 0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Evaluate the algorithms\n",
    "for name, algo in algos.items():\n",
    "    gs = GridSearchCV(algo, param_grid.get(name), measures=['rmse'], cv=cv, refit=True, n_jobs=-1)\n",
    "    gs.fit(trainset)\n",
    "    predicitons = gs.test(testset)\n",
    "    final_score = accuracy.rmse(predicitons)\n",
    "    benchmark.append([name, final_score, gs.best_params])\n",
    "\n",
    "results = pd.DataFrame(benchmark, columns=[\"Algorithm\", \"Final_RMSE\", \"Params\"]).sort_values(\"Final_RMSE\")\n",
    "results.set_index(\"Algorithm\", inplace=True)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
