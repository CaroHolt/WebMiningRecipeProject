{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bm4nwhQcFy1t"
   },
   "source": [
    "# Recommender System for Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO for FINAL-FINAL-Version:\n",
    "- Evaluation Test Set (Hybrid Model against Baseline Model with all metrics incl. RMSE) - Prio 1\n",
    "- <s>Schönheitsfehler im Code - Prio 3 DONE\n",
    "    Imports strukturieren (+ alle nach oben) DONE\n",
    "    Kommentare DONE\n",
    "    Output in Console DONE\n",
    "    Kapitel-Nummerierung: 1.1 vs. 2.0 & Hybrid Modell Kapitel in ToC einfügen DONE</s>\n",
    "- Tabelle zum Hybrid Model: Weights -> Evaluation Measures (Classification Metric Tabelle) - Prio 1\n",
    "- <s>Fehler in Auswahl des besten KNNModells (KNNwithMeans anstatt KNNBaseline) - Prio 1 DONE</s>\n",
    "- <s>Description bei Content-based rausnehmen, da RMSE verschlechtert - Prio 2 DONE</s>\n",
    "- <s>Information am Anfang \"Functions with minor relevance\"- Prio 3 DONE</s>\n",
    "- RMSE zu Plain Cosine berechnen - Prio 1/2\n",
    "- Kommentare zu Fit the Model -> hinweis, dass bei Evaluation gefittet wird - Prio 3\n",
    "- Hard Coded Backup der Hyperparameter überarbeiten - Prio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9J7hNsUFy1u"
   },
   "source": [
    "## Table of Contents\n",
    "[1. Data Preprocessing and Imports](#preprocessing) <br>\n",
    "    [1.1. Recipe filter/sampler](#filter_recipes) <br>\n",
    "    [1.2. User filter/sampler](#filter_users) <br>\n",
    "    [1.3. Interaction filter/sampler](#filter_interactions) <br>\n",
    "    [1.4. NLP Preprocessing](#nlp-preprocessing) <br>\n",
    "    [1.5. Preprocessing Summary](#summary_preprocessing) <br>\n",
    "    \n",
    "[2. Train/Test Split](#train_test_split) <br>\n",
    "\n",
    "[3. Models](#models) <br>\n",
    "    [3.1. General functions](#general-functions) <br>\n",
    "        [3.1.1. Recommendations functions for Coverage & Personalization](#recommendation-functions) <br>\n",
    "        [3.1.2. Prediction function for RMSE](#prediction-functions) <br>\n",
    "    [3.2 Content-Based Models](#content-based) <br>\n",
    "    [3.2.1 Cosine Similarity](#cosine) <br>\n",
    "        [3.2.1.1 Tfidf & SVD](#tfidf-svd) <br>\n",
    "        [3.2.1.2 WordEmbeddings](#word-embeddings) <br>\n",
    "    [3.2.2 Mixture Model](#mixture) <br>\n",
    "    [3.3 Collaborative Filtering Models](#collab_filt) <br>\n",
    "    [3.3 Hybrid Model](#hybrid_model) <br>\n",
    "    \n",
    "[4. Interpretation and Evaluation](#evaluation) <br>\n",
    "    [4.1 Single Algorithms Classification Metrics](#single_algos) <br>\n",
    "    [4.2 Final Scores](#final_scores) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opSpfJueFy1v"
   },
   "source": [
    "<a id='preprocessing'></a>\n",
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAWkwYp3Fy1w"
   },
   "source": [
    "##### Packages to install in cmd upfront:\n",
    "\n",
    "conda install -c conda-forge selenium <br>\n",
    "conda install -c anaconda nltk <br>\n",
    "conda install -c conda-forge progressbar <br>\n",
    "conda install -c conda-forge scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1590065369498,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "tYxs8eCoFy1w",
    "outputId": "f9aac27b-c260-473f-be57-96bd2f8d8952"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import inflect\n",
    "import datetime\n",
    "import time\n",
    "import re, string, unicodedata\n",
    "import progressbar\n",
    "import collections\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from datetime import datetime\n",
    "from progressbar import ProgressBar\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.metrics import jaccard_score, pairwise_distances_chunked, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from surprise import SVD, NMF, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline, NormalPredictor, BaselineOnly, CoClustering, SlopeOne\n",
    "from surprise import Dataset, Reader, accuracy, Trainset\n",
    "from surprise.model_selection import cross_validate, GridSearchCV, ShuffleSplit\n",
    "import gensim.downloader as api\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions module (Includes Preprocessing and Evaluation functionalities)\n",
    "%run functions.py\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fl3uG0AFy16"
   },
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNiZDdmCFy16"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "interactions_raw = pd.read_csv('Data/RAW_interactions.csv')\n",
    "recipes_raw = pd.read_csv('Data/RAW_recipes.csv', parse_dates=['submitted'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uc-gXIJ6Fy1-"
   },
   "outputs": [],
   "source": [
    "#make copies so that we don't have to reload the data after mistakes\n",
    "interactions_data = interactions_raw.copy()\n",
    "recipes_data = recipes_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXsKgWozFy2B"
   },
   "outputs": [],
   "source": [
    "# Rename column in recipe data frame to have matching column names for the recipe id between recipe dataframe and\n",
    "# interaction data frame\n",
    "recipes_data.rename(columns={\"id\": \"recipe_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRQDIOOgFy2E"
   },
   "outputs": [],
   "source": [
    "# Deal with missing values and errors in data set\n",
    "# - Drop recipes without title\n",
    "# - Replace missing descriptions with empty string\n",
    "# - Replace missing minutes of a recipe if those can be found in the text\n",
    "# - Drop fake recipes (e.g. \"How to preserve a husband\")\n",
    "deal_with_NAs(recipes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFie2-z9Fy2I"
   },
   "source": [
    "<a id='filter_recipes'></a>\n",
    "### 1.1. Filter recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2507,
     "status": "ok",
     "timestamp": 1590065439024,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "2M1G9-d4Fy2J",
    "outputId": "7a02a21b-40da-4b22-95ad-f6924ff6d2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after filtering recipes with less than 10 interactions and older than 10 years old: (48605, 15)\n",
      "Shape after filtering recipes with less than 5 interactions and younger than 10 years old: (20487, 15)\n",
      "Shape after removing 1 step recipes w/ low interactions: (20487, 15)\n",
      "Shape after removing recipes w/o ratings: (20487, 15)\n",
      "Shape after removing 0 minutes interaction w/ low interactions: (20397, 15)\n",
      "Shape after dropping duplicates: (20371, 15)\n",
      "URLs created for each of the 20371 recipes\n"
     ]
    }
   ],
   "source": [
    "# Filter the recipes (filtering is carried out inplace)\n",
    "\n",
    "# Add age and average rating required for filtering\n",
    "recipes_data['age'] = round((2019-recipes_data.submitted.dt.year)+recipes_data.submitted.dt.month/12, 1)\n",
    "recipes_data = get_avg_recipe_rating(interactions_data, recipes_data)\n",
    "# Filter based on number of interactions\n",
    "# Recipes older than 10 years need to have at least ten ratings\n",
    "filter_byinteractions(10,10,recipes_data, older=True)\n",
    "# Recipes younger than ten years need to have at least 5 ratings\n",
    "filter_byinteractions(5,10,recipes_data, older=False)\n",
    "# Filter based on qualitative criteria\n",
    "filter_byquality(recipes_data)\n",
    "# Remove recipes that have the same title\n",
    "remove_duplicates(recipes_data)\n",
    "# Generate the URL to the foods.com website for remaining recipes\n",
    "recipes_data = generate_URL(recipes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1590065444495,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "ka21uSe6Fy2N",
    "outputId": "1a3938e5-88d0-4fad-9b45-3940f370ccea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20371, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUmIRMgLFy2Q"
   },
   "source": [
    "<a id='filter_users'></a>\n",
    "### 1.2 Filter users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crs9vAHnFy2Q"
   },
   "source": [
    "Filter data flow:\n",
    "\n",
    "filter_interactions_data() -(calls)-> \n",
    "    (impute_average_rating(), (create_activity_data() \n",
    "                                        -(calls)-> get_user_activity_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rM8-IO5QFy2R"
   },
   "outputs": [],
   "source": [
    "def impute_average_rating(row, df_uactivity):\n",
    "    if (row['rating'] == 0):\n",
    "        imputed_rating = round(df_uactivity.loc[df_uactivity.user_id == row.user_id, 'uavg_rating'].values[0], 0)\n",
    "        return imputed_rating\n",
    "    else:\n",
    "        return row.rating\n",
    "    \n",
    "def get_user_activity_df(interactions):\n",
    "    #Create a user activity dataframe\n",
    "    df_uactivity = interactions.groupby('user_id')['rating'].value_counts().unstack().fillna(0)\n",
    "    cols = list(df_uactivity)\n",
    "    df_uactivity['total_interactions'] = df_uactivity[cols].sum(axis=1)\n",
    "    df_uactivity['total_ratings'] = df_uactivity['total_interactions']-df_uactivity[0]\n",
    "    return df_uactivity\n",
    "\n",
    "def get_user_behavior(filtered_user_activity):\n",
    "    #calculate arithmetic mean of ratings for each user\n",
    "    filtered_user_activity['uavg_rating'] = filtered_user_activity.iloc[:,1:6].apply(\n",
    "        lambda row: np.round(np.ma.average(list(range(1,6)), \n",
    "                                           weights = (row[1], row[2], row[3], row[4], row[5])),1), axis = 1)\n",
    "    filtered_user_activity = filtered_user_activity.reset_index()\n",
    "    filtered_user_activity.columns.set_names(None, inplace = True)\n",
    "    return filtered_user_activity\n",
    "    \n",
    "def filter_users(interactions_df, thresh_high, thresh_low):\n",
    "    df_uactivity = get_user_activity_df(interactions_df[['recipe_id','user_id', 'rating']])\n",
    "    filtered_user_activity = df_uactivity.loc[(df_uactivity['total_interactions']>=thresh_low) &\n",
    "                                              (df_uactivity['total_interactions']<thresh_high), :]\n",
    "    \n",
    "    print(f'Number of users after filtering out users with less than {thresh_low} interactions: {len(filtered_user_activity)}')\n",
    "    \n",
    "    #drop users that only have only reviews but no ratings\n",
    "    filtered_user_activity.drop(filtered_user_activity[filtered_user_activity.total_ratings == 0].index, \n",
    "                                inplace=True, \n",
    "                                axis=0)\n",
    "    \n",
    "    #create average user ratings as behavior\n",
    "    filtered_user_activity = get_user_behavior(filtered_user_activity)\n",
    "    \n",
    "    return filtered_user_activity\n",
    "\n",
    "def filter_interactions_data(interactions_df, recipes_data, thresh_high, thresh_low):\n",
    "    \n",
    "    # 2. Filter interactions with active recipes -> Only interactions from active users and clean recipes remain\n",
    "    interactions = interactions_df[interactions_df.recipe_id.isin(recipes_data.recipe_id)]\n",
    "    \n",
    "    filtered_user_activity = filter_users(interactions, thresh_high, thresh_low)\n",
    "    # 1. Get user ids of active\n",
    "    interactions = pd.merge(filtered_user_activity[['user_id', 'uavg_rating', 'total_interactions']], \n",
    "                                        interactions[['recipe_id','user_id', 'rating']], \n",
    "                                        how = 'left', on ='user_id')\n",
    "    \n",
    "    #impute ratings for interactions without ratings\n",
    "    zero_ratings = len(interactions.loc[interactions.rating==0])\n",
    "    interactions['rating'] = interactions.apply(lambda row: impute_average_rating(row, filtered_user_activity), axis=1)\n",
    "    print(f'Number of imputed ratings: {zero_ratings}')\n",
    "\n",
    "    return interactions[['user_id','recipe_id','rating','uavg_rating', 'total_interactions',]], filtered_user_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='filter_interactions'></a>\n",
    "### 1.3 Fiiter interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12135,
     "status": "ok",
     "timestamp": 1590065462733,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "-gn07BnmFy2U",
    "outputId": "1491de43-2d0d-406a-b832-7e4666b9c710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users after filtering out users with less than 9 interactions: 8436\n",
      "Number of imputed ratings: 5709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>uavg_rating</th>\n",
       "      <th>total_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533</td>\n",
       "      <td>96621</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1533</td>\n",
       "      <td>2137</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1533</td>\n",
       "      <td>132916</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1533</td>\n",
       "      <td>13813</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533</td>\n",
       "      <td>57549</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating  uavg_rating  total_interactions\n",
       "0  1533     96621      5.0     4.8          32.0              \n",
       "1  1533     2137       5.0     4.8          32.0              \n",
       "2  1533     132916     5.0     4.8          32.0              \n",
       "3  1533     13813      5.0     4.8          32.0              \n",
       "4  1533     57549      5.0     4.8          32.0              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204223, 5)\n"
     ]
    }
   ],
   "source": [
    "# Filter interactions data such that only interaction with active users (at least 7 ratings, but not more than 100) \n",
    "# and with recipes that were not filtered out before remain\n",
    "interactions, activity = filter_interactions_data(interactions_data, recipes_data, thresh_high=100, thresh_low=9)\n",
    "display(interactions.head())\n",
    "print(interactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1590065466434,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "P1u5yaztFy2X",
    "outputId": "4fec0684-70b8-4609-807f-eb130dbe516a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8431.000000\n",
      "mean     23.545724  \n",
      "std      18.353140  \n",
      "min      2.000000   \n",
      "25%      11.000000  \n",
      "50%      16.000000  \n",
      "75%      29.000000  \n",
      "max      99.000000  \n",
      "Name: total_ratings, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of number of ratings per user, to validate the upper limit of ratings for filtering the users\n",
    "activity['total_ratings'].plot(kind='box', vert=False)\n",
    "print(activity.total_ratings.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1590065476830,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "mHeL6AhiFy2a",
    "outputId": "51c5f93a-dd6e-462f-c088-367a1d17dfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of 5 star rating interactions: 79.22%\n",
      "Percent of 4 star rating interactions: 15.36%\n",
      "Percent of 3 star rating interactions: 3.6%\n",
      "Percent of 2 star rating interactions: 1.23%\n",
      "Percent of 1 star rating interactions: 0.59%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's look at the distribution of ratings\n",
    "get_rating_dist(interactions.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quCOd1fEFy2f"
   },
   "source": [
    "<a id='nlp-preprocessing'></a>\n",
    "### 1.4. Preprocessing for recipe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZoLdoXrFy2f"
   },
   "outputs": [],
   "source": [
    "# Function to concatenate the textual data per recipe to one string used for the content based approach\n",
    "def create_input (df, column_names):\n",
    "    df_content = df\n",
    "    df_content = df_content.assign(content= df.loc[:, (column_names)].apply(lambda texts: ' '.join(texts), axis=1))\n",
    "    df_content = df_content.drop(columns = column_names)\n",
    "    df_content['content']=df_content['content'].apply(lambda text: ' '.join(text.split()))\n",
    "    return df_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1590065484414,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "N-OZ-ISOFy2i",
    "outputId": "4efe0259-a3d8-4bcf-eaef-902ea4d6ed1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>63986</td>\n",
       "      <td>chicken lickin good pork chops dredge pork chops in mixture of flour salt dry mustard and garlic powder brown in oil in a large skillet place browned pork chops in a crock pot add the can of soup undiluted cover and cook on low for 6-8 hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23933</td>\n",
       "      <td>chinese candy melt butterscotch chips in heavy saucepan over low heat fold in peanuts and chinese noodles until coated drop by tablespoon onto waxed paper let stand in cool place until firm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>54100</td>\n",
       "      <td>grilled venison burgers in bowl mix dry ingredients add venison and mix well add liquid ingredients and mix well with a fork until bread crumbs are barely noticeable on plastic wrap form into 8 patties making them round and very flat-- like a fast-food burger they can be cooked immediately but i prefer to freeze them first place them on a large cookie sheet and freeze them several hours remove from freezer and place waxed paper between each burger as you stack them i stack them in 2 1-quart freezer zip-lock bags preheat cast iron skillet to very hot cook frozen burgers until they are slightly charred on one side and then turn and cook until the other side is slightly charred and burgers are medium-well do not overcook turn burner off and leave burgers in the pan while you put condiments on your hamburger bun if cooking thawed burgers cook about 2-3 minutes per side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>67664</td>\n",
       "      <td>healthy for them yogurt popsicles mix all the ingredients using a blender pour into popsicle molds freeze and enjoy !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25775</td>\n",
       "      <td>how i got my family to eat spinach spinach casserole preheat oven to 350 degrees place spinach in strainer and squeeze all extra liquid from it in a large bowl combine spinach with the rest of ingredients except croutons and combine well pour into a 2 quart casserole dish and top with croutons bake for 35 to 40 minutes or until bubbling around edges</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    recipe_id  \\\n",
       "15  63986       \n",
       "17  23933       \n",
       "33  54100       \n",
       "34  67664       \n",
       "36  25775       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          content  \n",
       "15  chicken lickin good pork chops dredge pork chops in mixture of flour salt dry mustard and garlic powder brown in oil in a large skillet place browned pork chops in a crock pot add the can of soup undiluted cover and cook on low for 6-8 hours                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "17  chinese candy melt butterscotch chips in heavy saucepan over low heat fold in peanuts and chinese noodles until coated drop by tablespoon onto waxed paper let stand in cool place until firm                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "33  grilled venison burgers in bowl mix dry ingredients add venison and mix well add liquid ingredients and mix well with a fork until bread crumbs are barely noticeable on plastic wrap form into 8 patties making them round and very flat-- like a fast-food burger they can be cooked immediately but i prefer to freeze them first place them on a large cookie sheet and freeze them several hours remove from freezer and place waxed paper between each burger as you stack them i stack them in 2 1-quart freezer zip-lock bags preheat cast iron skillet to very hot cook frozen burgers until they are slightly charred on one side and then turn and cook until the other side is slightly charred and burgers are medium-well do not overcook turn burner off and leave burgers in the pan while you put condiments on your hamburger bun if cooking thawed burgers cook about 2-3 minutes per side  \n",
       "34  healthy for them yogurt popsicles mix all the ingredients using a blender pour into popsicle molds freeze and enjoy !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "36  how i got my family to eat spinach spinach casserole preheat oven to 350 degrees place spinach in strainer and squeeze all extra liquid from it in a large bowl combine spinach with the rest of ingredients except croutons and combine well pour into a 2 quart casserole dish and top with croutons bake for 35 to 40 minutes or until bubbling around edges                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten recipe´s steps\n",
    "content_processed = recipes_data.copy()\n",
    "content_processed.steps = content_processed.loc[:, ('steps')].str.replace(\"\\[\", \"\").str.replace(\"'\", \"\").str.replace(\"\\]\", \"\").str.replace(\",\",\"\").copy()\n",
    "\n",
    "# Concatenate recipe name and steps to one string per recipe -> name and steps are most meaningful (steps most likely also include ingredients)\n",
    "content_processed = create_input(content_processed[['recipe_id', 'name', 'steps']], ['name', 'steps'])\n",
    "\n",
    "content_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103139,
     "status": "ok",
     "timestamp": 1590065588753,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "VWTN5eq4Fy2n",
    "outputId": "e723978d-7dc6-45a1-e035-058d4ffcac19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20371, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run NLP-preprocessing on the recipe content\n",
    "content_processed = get_processed(content_processed)\n",
    "content_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sr7WrwPqFy2q"
   },
   "source": [
    "<a id='summary_preprocessing'></a>\n",
    "### 1.5 Remaining data size (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1590065593471,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "y_8D1m-vFy2r",
    "outputId": "d701832a-f47f-4f74-e847-7bd238b70f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipes: 20371\n",
      "Number of users: 8431\n",
      "Number of interactions: 204223\n"
     ]
    }
   ],
   "source": [
    "n_recipes = len(content_processed)\n",
    "n_users = len(interactions.user_id.unique())\n",
    "print(f'Number of recipes: {n_recipes}')\n",
    "print(f'Number of users: {n_users}')\n",
    "print(f'Number of interactions: {len(interactions)}')\n",
    "\n",
    "# Free some memory\n",
    "del interactions_raw, recipes_raw, interactions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJX4iiEgFy2z"
   },
   "source": [
    "<a id='train_test_split'></a>\n",
    "## 2. Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7eIQj6JFy20"
   },
   "outputs": [],
   "source": [
    "def get_fixedN_test_train_split(interactions, n):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---------\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    test_split: DataFrame\n",
    "        Interactions DataFrame containing only n interactions per user\n",
    "    train_split: DataFrame\n",
    "        Interactions DataFrame containing the rest of the interactions\n",
    "    \"\"\"\n",
    "    \n",
    "    test_split = pd.DataFrame(columns = ['user_id', 'recipe_id', 'rating'])\n",
    "    groups = interactions.groupby('user_id')\n",
    "    \n",
    "    #with progressbar.ProgressBar(max_value=len(interactions.user_id.drop_duplicates().values)) as bar:\n",
    "    for i, group in groups:\n",
    "        test_shard = group.sample(n=n, random_state=10)\n",
    "        test_split = pd.concat([test_split, test_shard], ignore_index=True)\n",
    "            \n",
    "    interactions = interactions.set_index(['user_id', 'recipe_id']).sort_index()    \n",
    "    test_split = test_split.set_index(['user_id', 'recipe_id']).sort_index()\n",
    "    train_split = interactions[~interactions.index.isin(test_split.index)]\n",
    "    \n",
    "    train_split.reset_index(inplace=True)\n",
    "    test_split.reset_index(inplace=True)\n",
    "    \n",
    "    \n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8C4ExitFy22"
   },
   "outputs": [],
   "source": [
    "#Create train-test split of interaction data by keeping a fixed number of interactions per user for testing\n",
    "fixed_n_test = 3\n",
    "train_split, test_split = get_fixedN_test_train_split(interactions, fixed_n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15732,
     "status": "ok",
     "timestamp": 1590065616311,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "HCYvGvqOFy25",
    "outputId": "af21a392-75fa-4e64-8cde-d8443afa8030",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 14.14% \n",
      "\n",
      "Rating distribution in test set \n",
      "\n",
      "Percent of 5 star rating interactions: 79.6%\n",
      "Percent of 4 star rating interactions: 14.72%\n",
      "Percent of 3 star rating interactions: 3.61%\n",
      "Percent of 2 star rating interactions: 1.41%\n",
      "Percent of 1 star rating interactions: 0.66%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD+CAYAAADYr2m5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAVfklEQVR4nO3df5Dc9X3f8efLwjD+EYwIh6pIckRc2S2miQAFNOPaQ0oBgT0W7oQW2jGqSyPbhak9zh+Wk+ng2GWGtnHcYcbBkYOMmLEhxJhBiUWJovGPcQtYB1b5aarjh80hWTpbjiHFg0f43T/2c9Vy2pNOt6fbw/d8zOzsd9/fz3f3vd/R6nXfH7vfVBWSpPntNYNuQJI0eIaBJMkwkCQZBpIkDANJEnDcoBuYrlNOOaWWL18+6DYk6VXlgQce+FFVDU2sv2rDYPny5QwPDw+6DUl6VUny/V51dxNJkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkphAGSZYl+XqSx5M8muQjrX5ykm1JdrX7ha2eJDckGUnyUJKzup5rXRu/K8m6rvrZSR5uy9yQJMfizUqSepvKlsEB4Per6h8Dq4Grk5wObAC2V9UKYHt7DHAxsKLd1gM3Qic8gGuBc4FzgGvHA6SNWd+13Jr+35okaaqO+A3kqtoD7GnTLyR5HFgCrAXOa8M2A98APt7qt1Tnqjn3JTkpyeI2dltV7QdIsg1Yk+QbwIlVdW+r3wJcCtw9M29xcss3fO1Yv8QRPXP9uwfdgiQd3TGDJMuBM4H7gUUtKMYD49Q2bAnwbNdio612uPpoj3qv11+fZDjJ8NjY2NG0Lkk6jCmHQZI3AncAH62q5w83tEetplE/tFi1sapWVdWqoaFDfmdJkjRNUwqDJK+lEwRfqqqvtvLetvuHdr+v1UeBZV2LLwV2H6G+tEddkjRLpnI2UYCbgMer6k+6Zm0Bxs8IWgfc1VW/sp1VtBr4aduNdA9wYZKF7cDxhcA9bd4LSVa317qy67kkSbNgKj9h/Q7g/cDDSXa22h8A1wO3J7kK+AFwWZu3FbgEGAFeBD4AUFX7k3wa2NHGfWr8YDLwYeBm4HV0Dhwf84PHkqSDpnI20bfpvV8f4Pwe4wu4epLn2gRs6lEfBs44Ui+SpGPDbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJqV0DeVOSfUke6ar9RZKd7fbM+OUwkyxP8rOueZ/vWubsJA8nGUlyQ7veMUlOTrItya52v/BYvFFJ0uSmsmVwM7Cmu1BV/6qqVlbVSuAO4Ktds58cn1dVH+qq3wisB1a02/hzbgC2V9UKYHt7LEmaRUcMg6r6FrC/17z21/2/BG493HMkWQycWFX3tmsk3wJc2mavBTa36c1ddUnSLOn3mME7gb1VtaurdlqS7yb5ZpJ3ttoSYLRrzGirASyqqj0A7f7UyV4syfokw0mGx8bG+mxdkjSu3zC4glduFewB3lxVZwIfA76c5EQgPZato32xqtpYVauqatXQ0NC0GpYkHeq46S6Y5DjgXwBnj9eq6iXgpTb9QJIngbfS2RJY2rX4UmB3m96bZHFV7Wm7k/ZNtydJ0vT0s2Xwz4HvVdX/3/2TZCjJgjb9G3QOFD/Vdv+8kGR1O85wJXBXW2wLsK5Nr+uqS5JmyVROLb0VuBd4W5LRJFe1WZdz6IHjdwEPJfnfwFeAD1XV+MHnDwN/DowATwJ3t/r1wAVJdgEXtMeSpFl0xN1EVXXFJPV/26N2B51TTXuNHwbO6FH/MXD+kfqQJB07fgNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJKY2pXONiXZl+SRrtonkzyXZGe7XdI17xNJRpI8keSirvqaVhtJsqGrflqS+5PsSvIXSY6fyTcoSTqyqWwZ3Ays6VH/bFWtbLetAElOp3M5zLe3Zf40yYJ2XeTPARcDpwNXtLEA/6U91wrgJ8BVE19IknRsHTEMqupbwP4jjWvWArdV1UtV9TSd6x2f024jVfVUVf0cuA1YmyTAP6NzvWSAzcClR/keJEl96ueYwTVJHmq7kRa22hLg2a4xo602Wf1Xgb+rqgMT6pKkWTTdMLgReAuwEtgDfKbV02NsTaPeU5L1SYaTDI+NjR1dx5KkSU0rDKpqb1W9XFW/AL5AZzcQdP6yX9Y1dCmw+zD1HwEnJTluQn2y191YVauqatXQ0NB0Wpck9TCtMEiyuOvh+4DxM422AJcnOSHJacAK4DvADmBFO3PoeDoHmbdUVQFfB363Lb8OuGs6PUmSpu+4Iw1IcitwHnBKklHgWuC8JCvp7NJ5BvggQFU9muR24DHgAHB1Vb3cnuca4B5gAbCpqh5tL/Fx4LYk/xn4LnDTjL07SdKUHDEMquqKHuVJ/8OuquuA63rUtwJbe9Sf4uBuJknSAPgNZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJDGFMEiyKcm+JI901f5bku8leSjJnUlOavXlSX6WZGe7fb5rmbOTPJxkJMkNSdLqJyfZlmRXu194LN6oJGlyU9kyuBlYM6G2DTijqn4T+D/AJ7rmPVlVK9vtQ131G4H1wIp2G3/ODcD2qloBbG+PJUmz6IhhUFXfAvZPqP1NVR1oD+8Dlh7uOZIsBk6sqnurqoBbgEvb7LXA5ja9uasuSZolM3HM4N8Bd3c9Pi3Jd5N8M8k7W20JMNo1ZrTVABZV1R6Adn/qZC+UZH2S4STDY2NjM9C6JAn6DIMkfwgcAL7USnuAN1fVmcDHgC8nORFIj8XraF+vqjZW1aqqWjU0NDTdtiVJExw33QWTrAPeA5zfdv1QVS8BL7XpB5I8CbyVzpZA966kpcDuNr03yeKq2tN2J+2bbk+SpOmZ1pZBkjXAx4H3VtWLXfWhJAva9G/QOVD8VNv980KS1e0soiuBu9piW4B1bXpdV12SNEuOuGWQ5FbgPOCUJKPAtXTOHjoB2NbOEL2vnTn0LuBTSQ4ALwMfqqrxg88fpnNm0uvoHGMYP85wPXB7kquAHwCXzcg7kyRN2RHDoKqu6FG+aZKxdwB3TDJvGDijR/3HwPlH6kOSdOz4DWRJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCQxxTBIsinJviSPdNVOTrItya52v7DVk+SGJCNJHkpyVtcy69r4XUnWddXPTvJwW+aGdp1kSdIsmeqWwc3Amgm1DcD2qloBbG+PAS4GVrTbeuBG6IQHnesnnwucA1w7HiBtzPqu5Sa+liTpGJpSGFTVt4D9E8prgc1tejNwaVf9luq4DzgpyWLgImBbVe2vqp8A24A1bd6JVXVvVRVwS9dzSZJmQT/HDBZV1R6Adn9qqy8Bnu0aN9pqh6uP9qgfIsn6JMNJhsfGxvpoXZLU7VgcQO61v7+mUT+0WLWxqlZV1aqhoaE+WpQkdesnDPa2XTy0+32tPgos6xq3FNh9hPrSHnVJ0izpJwy2AONnBK0D7uqqX9nOKloN/LTtRroHuDDJwnbg+ELgnjbvhSSr21lEV3Y9lyRpFhw3lUFJbgXOA05JMkrnrKDrgduTXAX8ALisDd8KXAKMAC8CHwCoqv1JPg3saOM+VVXjB6U/TOeMpdcBd7ebJGmWTCkMquqKSWad32NsAVdP8jybgE096sPAGVPpRZI08/wGsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk0UcYJHlbkp1dt+eTfDTJJ5M811W/pGuZTyQZSfJEkou66mtabSTJhn7flCTp6EzpSme9VNUTwEqAJAuA54A76Vzm8rNV9cfd45OcDlwOvB34NeBvk7y1zf4ccAEwCuxIsqWqHptub5KkozPtMJjgfODJqvp+55r2Pa0Fbquql4Cnk4wA57R5I1X1FECS29pYw0CSZslMHTO4HLi16/E1SR5KsinJwlZbAjzbNWa01SarS5JmSd9hkOR44L3AX7bSjcBb6OxC2gN8Znxoj8XrMPVer7U+yXCS4bGxsb76liQdNBNbBhcDD1bVXoCq2ltVL1fVL4AvcHBX0CiwrGu5pcDuw9QPUVUbq2pVVa0aGhqagdYlSTAzYXAFXbuIkizumvc+4JE2vQW4PMkJSU4DVgDfAXYAK5Kc1rYyLm9jJUmzpK8DyEleT+csoA92lf9rkpV0dvU8Mz6vqh5NcjudA8MHgKur6uX2PNcA9wALgE1V9Wg/fUmSjk5fYVBVLwK/OqH2/sOMvw64rkd9K7C1n14kSdPnN5AlSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJLEDIRBkmeSPJxkZ5LhVjs5ybYku9r9wlZPkhuSjCR5KMlZXc+zro3flWRdv31JkqZuprYMfqeqVlbVqvZ4A7C9qlYA29tjgIuBFe22HrgROuEBXAucC5wDXDseIJKkY+9Y7SZaC2xu05uBS7vqt1THfcBJSRYDFwHbqmp/Vf0E2AasOUa9SZImmIkwKOBvkjyQZH2rLaqqPQDt/tRWXwI827XsaKtNVn+FJOuTDCcZHhsbm4HWJUkAx83Ac7yjqnYnORXYluR7hxmbHrU6TP2VhaqNwEaAVatWHTJfkjQ9fW8ZVNXudr8PuJPOPv+9bfcP7X5fGz4KLOtafCmw+zB1SdIs6CsMkrwhya+MTwMXAo8AW4DxM4LWAXe16S3Ale2sotXAT9tupHuAC5MsbAeOL2w1SdIs6Hc30SLgziTjz/XlqvofSXYAtye5CvgBcFkbvxW4BBgBXgQ+AFBV+5N8GtjRxn2qqvb32ZskaYr6CoOqegr4rR71HwPn96gXcPUkz7UJ2NRPP5Kk6fEbyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJLEzPxQnX4JLN/wtUG3wDPXv3vQLUjzllsGkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkkQfYZBkWZKvJ3k8yaNJPtLqn0zyXJKd7XZJ1zKfSDKS5IkkF3XV17TaSJIN/b0lSdLR6ucbyAeA36+qB5P8CvBAkm1t3mer6o+7Byc5HbgceDvwa8DfJnlrm/054AJgFNiRZEtVPdZHb5KkozDtMKiqPcCeNv1CkseBJYdZZC1wW1W9BDydZAQ4p80baddTJsltbaxhIEmzZEaOGSRZDpwJ3N9K1yR5KMmmJAtbbQnwbNdio602Wb3X66xPMpxkeGxsbCZalyQxA2GQ5I3AHcBHq+p54EbgLcBKOlsOnxkf2mPxOkz90GLVxqpaVVWrhoaG+m1dktT09aulSV5LJwi+VFVfBaiqvV3zvwD8dXs4CizrWnwpsLtNT1aXJM2Cfs4mCnAT8HhV/UlXfXHXsPcBj7TpLcDlSU5IchqwAvgOsANYkeS0JMfTOci8Zbp9SZKOXj9bBu8A3g88nGRnq/0BcEWSlXR29TwDfBCgqh5NcjudA8MHgKur6mWAJNcA9wALgE1V9WgffUmSjlI/ZxN9m977+7ceZpnrgOt61LcebjlJ0rHlN5AlSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk0edPWEu/bJZv+NqgWwDgmevfPegWNM+4ZSBJMgwkSYaBJAnDQJKEB5AlTcKD6fPLnNkySLImyRNJRpJsGHQ/kjSfzIktgyQLgM8BFwCjwI4kW6rqscF2JknzYytprmwZnAOMVNVTVfVz4DZg7YB7kqR5I1U16B5I8rvAmqr69+3x+4Fzq+qaCePWA+vbw7cBT8xqo4c6BfjRgHuYK1wXB7kuDnJdHDRX1sWvV9XQxOKc2E0EpEftkJSqqo3AxmPfztQkGa6qVYPuYy5wXRzkujjIdXHQXF8Xc2U30SiwrOvxUmD3gHqRpHlnroTBDmBFktOSHA9cDmwZcE+SNG/Mid1EVXUgyTXAPcACYFNVPTrgtqZizuyymgNcFwe5Lg5yXRw0p9fFnDiALEkarLmym0iSNECGgSTJMJAkGQbqU5KTkywcdB9zgetCr2aGwVFKsijJWUnOTLJo0P0MQpI3J7ktyRhwP53fktrXassH293scl0cys/Iq5NnE01RkpXA54E3Ac+18lLg74D/UFUPDqq32ZbkXuC/A1+pqpdbbQFwGfDRqlo9yP5mk+viID8jh2phuITOLyrsrqq9A25pUobBFCXZCXywqu6fUF8N/FlV/dZgOpt9SXZV1YqjnffLyHVxkJ+Rg16NwTgnvnT2KvGGif/IAarqviRvGERDA/RAkj8FNgPPttoyYB3w3YF1NRiui4P8jBx0M5MH4xeBOReMbhlMUZIbgLcAt/DKD/2VwNMTf2H1l1n7yZCr6PzM+BI6PzT4LPBXwE1V9dIA25tVrouD/IwcdIQtxpGq+oez3dORGAZHIcnFvPJDPwpsqaqtA21MmiP8jHS8GoPRMNCMSvKeqvrrQfcxF7gu5rdXWzB6zGAGJFnfrrUg+G3A/wA7XBfNfPyMVNXdwN2D7mOq/J7BzOh1cZ55JcktAFV17aB7mW1Jzkny22369CQfS3LJfFwXhzHvPyPj2hUb5xy3DKYpyT+lc+3mR6rqzwbdz2xKMvFaEwF+J8lJAFX13tnvajCSXAtcDByXZBtwLvANYEOSM6vqukH2N9uS/CM6u0Xur6q/75r1/QG1NBfNyWD0mMEUJflOVZ3Tpn8PuBq4E7gQ+Kuqun6Q/c2mJA8CjwF/TufLNAFupXNRIqrqm4PrbnYleRhYCZwA/BBYWlXPJ3kdnf8Qf3OgDc6iJP+RzuficTrr5CNVdVeb92BVnTXI/uaKJB+oqi8Ouo+J3E00da/tml4PXFBVf0QnDP7NYFoamFXAA8AfAj+tqm8AP6uqb86nIGgOVNXLVfUi8GRVPQ9QVT8DfjHY1mbd7wFnV9WlwHnAf0rykTZvTv41PCB/NOgGenE30dS9pv0I2WvobFGNAVTV/01yYLCtza6q+gXw2SR/2e73Mn//Lf08yetbGJw9XkzyJuZfGCwY3zVUVc8kOQ/4SpJfZ56FQZKHJpsFzMnfa5qvH+DpeBOdv4YDVJJ/UFU/TPJG5tk/9HFVNQpcluTdwPOD7mdA3jX+xbIWkuNeS+dbyPPJD5OsrKqdAFX190neA2wC/slgW5t1i4CLgJ9MqAf4X7PfzpF5zKBPSV4PLKqqpwfdizRISZbS2W32wx7z3lFV/3MAbQ1EkpuAL1bVt3vM+3JV/esBtHVYhoEkyQPIkiTDQJKEYSBJwjCQJAH/DxUXC920u2VDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate quality of train-test split\n",
    "print(f'Test size: {round((len(test_split)/len(train_split)) * 100,2)}% \\n')\n",
    "print('Rating distribution in test set \\n')\n",
    "get_rating_dist(test_split.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg4kEL3PFy28"
   },
   "source": [
    "<a id='models'></a>\n",
    "## 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdAgefBdFy28"
   },
   "source": [
    "<a id='general-functions'></a>\n",
    "###  General functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfcL8fhUFy29"
   },
   "source": [
    "<a id='recommendation-functions'></a>\n",
    "#### Recommendations functions for precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swM4DgC4Fy29"
   },
   "outputs": [],
   "source": [
    "#return top k predicted ratings in readable form \n",
    "\n",
    "# IMPORTANT: must set the dataframe for recipe_info index == recipe_id!\n",
    "def get_user_recommendations(user_id, similarity, content, interactions, recipe_info, k):\n",
    "    \"\"\"\n",
    "     Returns\n",
    "    ----------\n",
    "    info:\n",
    "        HTML dataframe with recommendation information\n",
    "    \"\"\"\n",
    "    #get top k recipe ids\n",
    "    topk_recipes, predictions, recipeurls, imageurls = get_topk_recipes(user_id, similarity, content, interactions, k)\n",
    "    info = recipe_info.loc[topk_recipes]\n",
    "    info = info[['name', 'minutes', 'submitted', 'description']]\n",
    "    info['prediction'] = predictions\n",
    "    info['recipeurl'] = recipeurls\n",
    "    info['imageurl'] = imageurls\n",
    "    for index, row in info.iterrows():\n",
    "        info.at[index, 'recipeurl'] = '<a href=\"'+ row['recipeurl'] + '\">'+ row['recipeurl'] + '</a>'\n",
    "        info.at[index, 'imageurl'] = '<a href=\"'+ row['imageurl'] + '\"> Image of recipe '+ str(index) + '</a>'\n",
    "    info = HTML(info.to_html(escape=False))\n",
    "    return info\n",
    "\n",
    "def get_topk_recipes(user_id, similarity, content, interactions, k):\n",
    "    \"\"\"\n",
    "     Returns\n",
    "    ----------\n",
    "    topk_recipes:\n",
    "        array of top k recipe ids\n",
    "    predictions:\n",
    "        array with top k predictions\n",
    "    recipeurls:\n",
    "        array with top k recipe urls\n",
    "    imageurls:\n",
    "        array with top k recipe imageurls\n",
    "    \"\"\"\n",
    "    prediction_df = get_user_preference(user_id,similarity, content, interactions)\n",
    "    #take only the not yet seen recipes\n",
    "    #new_predictions = prediction_df[prediction_df['has_rated'] == False]\n",
    "    #sort predictions\n",
    "    ordered_predictions = new_predictions.sort_values(by='prediction', ascending=False)\n",
    "    #get recipe_id array\n",
    "    topk_recipes = ordered_predictions.index[:k].values\n",
    "    imageurls = []\n",
    "    recipeurls = []\n",
    "    for entry in topk_recipes:\n",
    "        recipeurls.append(\"https://www.food.com/recipe/\" + str(entry))\n",
    "        imageurls.append(get_image_source_url(entry))\n",
    "    predictions = ordered_predictions.prediction[:k].values\n",
    "    return topk_recipes, predictions, recipeurls, imageurls\n",
    "\n",
    "#return predictions for 1 user\n",
    "def get_user_preference(user_id, similarity, content, interactions_data):\n",
    "    \"\"\"\n",
    "     Returns\n",
    "    ----------\n",
    "    prediction_df:\n",
    "        DataFrame in with columns ['recipe_id','prediction', 'has_rated'] for 1 user\n",
    "    \"\"\"\n",
    "    #prepare similarity dataframe\n",
    "    sim = pd.DataFrame(similarity, index = content['recipe_id'].values, columns = content['recipe_id'].values)\n",
    "    #get already rated recipes of user\n",
    "    rated_recipes = interactions_data.loc[interactions_data['user_id'] == user_id, 'recipe_id'].values\n",
    "    #get similarities of ALL recipes w/ already rated recipes of user\n",
    "    sim_rated_all = sim.loc[rated_recipes, :]\n",
    "    #get ratings of already rated recipes\n",
    "    ratings = get_reshaped_ratings(user_id, interactions_data)\n",
    "    \n",
    "    #compute weighted similarities between all recipes and already rated recipes\n",
    "    weighted_sim = np.dot(ratings,sim_rated_all)\n",
    "    #compute normalization constant\n",
    "    norm_const = np.array(np.abs(sim_rated_all).sum(axis=0))\n",
    "    #return sorted predictions\n",
    "    pref_predictions = weighted_sim/norm_const\n",
    "    \n",
    "    flat_predictions = [item for sublist in pref_predictions for item in sublist]\n",
    "    #return df with recipe id also\n",
    "    prediction_df = pd.DataFrame(flat_predictions, index = content['recipe_id'].values, columns = ['prediction'])\n",
    "    #indicate the already tried recipes\n",
    "    prediction_df['has_rated'] = prediction_df.index.isin(rated_recipes)\n",
    "    #order predictions\n",
    "    return prediction_df\n",
    "\n",
    "#arrange ratings for matrix multiplication\n",
    "def get_reshaped_ratings(user_id, interactions_data):\n",
    "    ratings = interactions_data.loc[interactions_data['user_id'] == user_id, :]\n",
    "    ratings.set_index('recipe_id', inplace=True)\n",
    "    ratings.index.set_names(None, inplace = True)\n",
    "    ratings = ratings.drop(columns = 'user_id')\n",
    "    ratings = ratings.transpose()\n",
    "    ratings.rename(index={'rating':user_id}, inplace=True)\n",
    "    return ratings.loc[ratings.index == user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ojgQ3AnFy3A"
   },
   "outputs": [],
   "source": [
    "def make_all_recommendations(user_ids, similarity, content, interactions, k):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    --------\n",
    "    user_ids: Array\n",
    "        list of user ids\n",
    "    similarity: Array \n",
    "        similarity matrix with shape (#recipes, #recipes).\n",
    "    content: DataFrame\n",
    "        processed DataFrame with ['recipe_id', 'content'] used to fetch all recipes ids to make recommendations for \n",
    "        (=total training data)\n",
    "    interactions: DataFrame\n",
    "        preprocessed interactions DataFrame ['recipe_id', 'user_id', 'rating']     \n",
    "    k: integer\n",
    "        number of recommendations to make\n",
    "    Returns:\n",
    "    --------\n",
    "    nested_recommendations:\n",
    "        nested list of recommended recipe_ids for each user in param list\n",
    "        example:[[rid1, rid20, rid30...], [rid1, rid20, rid30...],[rid1, rid20, rid30...]]\n",
    "    \"\"\"\n",
    "    pbar = ProgressBar()\n",
    "    nested_recommendations = {}\n",
    "    for i in pbar(range(len(user_ids))):\n",
    "        recs = get_topk_recipes_lean(user_ids[i], similarity, content, interactions, k)\n",
    "        nested_recommendations[user_ids[i]] = recs\n",
    "    return nested_recommendations\n",
    "\n",
    "def get_topk_recipes_lean(user_id, similarity, content, interactions, k):\n",
    "    \"\"\"\n",
    "     Returns\n",
    "    ----------\n",
    "    topk_recipes:\n",
    "        array of top k recipe ids\n",
    "    predictions:\n",
    "        array with top k predictions\n",
    "    recipeurls:\n",
    "        array with top k recipe urls\n",
    "    imageurls:\n",
    "        array with top k recipe imageurls\n",
    "    \"\"\"\n",
    "    prediction_df = get_user_preference(user_id, similarity, content, interactions)\n",
    "    #filter the already rated recipes (we do not recommend those)\n",
    "    prediction_df = prediction_df.loc[prediction_df.has_rated == False]\n",
    "    #sort predictions\n",
    "    ordered_predictions = prediction_df.sort_values(by='prediction', ascending=False)\n",
    "    #get recipe_id array\n",
    "    topk_recipes = ordered_predictions.index[:k].tolist()\n",
    "    predictions = ordered_predictions.prediction[:k].tolist()\n",
    "    return topk_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APXI8F13Fy3C"
   },
   "source": [
    "<a id='prediction-functions'></a>\n",
    "#### Prediction function for RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQvAsMVXFy3C"
   },
   "outputs": [],
   "source": [
    "#prediction for 1 already rated recipe based on similarities to other already rated recipes\n",
    "def get_one_prediction(similarity, content, interactions, user_id, recipe_id):\n",
    "    sim = pd.DataFrame(similarity, index=content['recipe_id'].values, columns=content['recipe_id'].values)\n",
    "    #get already rated recipes of user\n",
    "    rated_recipes = interactions[(interactions['user_id']==user_id) & (interactions['recipe_id']!=recipe_id)]['recipe_id'].values\n",
    "    #get similarities of to be predicted recipe rating with already rated recipes by user x\n",
    "    sim_rated = sim.loc[sim.index==recipe_id, rated_recipes].loc[recipe_id].values\n",
    "    #get ratings of rated recipes\n",
    "    ratings = interactions[(interactions['user_id']==user_id) & (interactions['recipe_id']!=recipe_id)]['rating'].values\n",
    "    \n",
    "    actual = interactions.loc[(interactions.user_id==user_id) & (interactions.recipe_id==recipe_id)]['rating'].values[0]\n",
    "    prediction = np.dot(ratings, sim_rated)/np.array([np.abs(sim_rated).sum(axis=0)])\n",
    "    return actual, prediction\n",
    "\n",
    "# Calls get_one_prediction for all users and recipes\n",
    "def make_all_predictions(num_interactions, similarity, content, interactions, uid_array, rids_array):\n",
    "    predictions_cos = []\n",
    "    actual_cos = []\n",
    "    mean_rating = interactions.rating.mean()\n",
    "    pbar = ProgressBar()\n",
    "    \n",
    "    for i in pbar(range(num_interactions)):\n",
    "        act, pred = get_one_prediction(similarity, content, interactions, uid_array[i], rids_array[i])\n",
    "        \n",
    "        if(np.isnan(pred[0])):\n",
    "            pred = mean_rating\n",
    "        \n",
    "        predictions_cos.append(pred)\n",
    "        actual_cos.append(act)\n",
    "        \n",
    "    return predictions_cos, actual_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_qL82TJFy3J"
   },
   "source": [
    "<a id='content_based'></a>\n",
    "### 3.2 Content-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RYfer7VFy3K"
   },
   "source": [
    "<a id='cosine'></a>\n",
    "### 3.2.1 Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffBlFASxFy3L"
   },
   "source": [
    "<a id='tfidf-svd'></a>\n",
    "#### 3.2.1.1 TFIDF/SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMjH0C-BFy3O"
   },
   "outputs": [],
   "source": [
    "def get_cos_sim_matrix(processed, n_components = 10, use_svd = True):\n",
    "    '''\n",
    "    Compute the cosine similarity matrix based on TF-IDF and SVD truncation\n",
    "    '''\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "    processed['content'] = processed['content'].fillna('')\n",
    "    tfidf_matrix = tfidf.fit_transform(processed['content'])\n",
    "    if use_svd:\n",
    "        #reduce dimensionality of tfidf matrix\n",
    "        svd = TruncatedSVD(n_components = n_components, random_state = 42)\n",
    "        tfidf_truncated = svd.fit_transform(tfidf_matrix)\n",
    "    else:\n",
    "        tfidf_truncated = tfidf_matrix\n",
    "    cosine_sim = cosine_similarity(tfidf_truncated, tfidf_truncated)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqLKcGY9Fy3Q"
   },
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPFJVoTyFy3R"
   },
   "outputs": [],
   "source": [
    "def tuning_cos_rmse(n_components, content, interactions, uid_array, rids_array):\n",
    "    '''\n",
    "    Function computes the RMSE for different number of components for the used SVD\n",
    "    '''\n",
    "    rmse_cos = []\n",
    "    \n",
    "    for n in n_components:\n",
    "        print(\"Computing predictions & recommendations for:\", n)\n",
    "        similarity = get_cos_sim_matrix(content, n)\n",
    "        \n",
    "        predictions, actuals = make_all_predictions(len(interactions), \n",
    "                                                    similarity, \n",
    "                                                    content, \n",
    "                                                    interactions, \n",
    "                                                    uid_array, \n",
    "                                                    rids_array)\n",
    "        rmse = mean_squared_error(actuals, predictions)**0.5\n",
    "        print(n, '=n_components', 'rmse:', rmse)\n",
    "        rmse_cos.append(rmse)\n",
    "        \n",
    "           \n",
    "    return rmse_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJcs3xRvFy3T",
    "outputId": "d15972eb-8c79-4a3a-9e69-9ab3f8122071"
   },
   "outputs": [],
   "source": [
    "# Define a suitable parameter range for the components of the SVD\n",
    "n_components = [50, 100, 200]\n",
    "# Tune the actual hyperparameters\n",
    "rmse_cos_tune = tuning_cos_rmse(n_components,\n",
    "                               content_processed,\n",
    "                               train_split,\n",
    "                               train_split['user_id'].values,\n",
    "                               train_split['recipe_id'].values, \n",
    "                               )\n",
    "# Get the hyperparameter with the lowest RMSE on the dev set\n",
    "rmse_cos_tune_min = min(rmse_cos_tune)\n",
    "rmse_cos_tune_min_idx = rmse_cos_tune.index(rmse_cos_tune_min)\n",
    "n_components_min = n_components[rmse_cos_tune_min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw7JbHyYFy3Z",
    "outputId": "1ecfdf2c-fbdb-40b2-ef7c-f69336851080"
   },
   "outputs": [],
   "source": [
    "# Plot the RMSE for different numbers of components\n",
    "plt.plot(n_components, rmse_cos_tune)\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for different tfidf/SVD models')\n",
    "plt.plot([n_components_min], [rmse_cos_tune_min], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6acKrORFy3c"
   },
   "source": [
    "#### Fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6qPb7VoFy3c"
   },
   "outputs": [],
   "source": [
    "# Proximity matrix of the TFIDF-SVD model\n",
    "# similarity_matrix_svd = get_cos_sim_matrix(content_processed, n_components_min, use_svd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkAxTnUjFy3f"
   },
   "outputs": [],
   "source": [
    "# Proximity matrix for the plain TFIDF model\n",
    "# similarity_matrix = get_cos_sim_matrix(content_processed, n_components_min, use_svd=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l74mpJJlFy3i"
   },
   "source": [
    "<a id='word-embeddings'></a>\n",
    "#### 3.2.1.2. WordEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7K7TMjECFy3i"
   },
   "source": [
    "#### Proximity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpv6MLW0Fy3j"
   },
   "outputs": [],
   "source": [
    "def normalize_tfidf(tfidf_vector, weights):\n",
    "    '''\n",
    "    Function computes the weighted average of a tf-idf vector\n",
    "    '''    \n",
    "    return(np.average(tfidf_vector, weights = weights, axis = 0))\n",
    "\n",
    "def get_tfidf_wordembedding_matrix(content_separated, vocabulary_name):\n",
    "    '''\n",
    "    Function generates word embedding matrix from pre-trained vocabulary and the recipe content separated by spaces\n",
    "    '''\n",
    "    # Load pre-trained vocabulary\n",
    "    model = api.load(vocabulary_name) # glove-wiki-gigaword-100 glove-wiki-gigaword-200 glove-wiki-gigaword-300\n",
    "    word2vector = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n",
    "    # Compute tf-idf vector\n",
    "    tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    tfidf.fit(content_separated)\n",
    "\n",
    "    # create dictionary with all vocabulary items and their tf-idf distance\n",
    "    max_idf = max(tfidf.idf_)\n",
    "    word2word = collections.defaultdict(\n",
    "        lambda: max_idf,\n",
    "        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "    # get representation of contents as tf-idf (second dimension used for weights)\n",
    "    dim = len(word2vector.items())\n",
    "    tfidf_matrix = np.array([\n",
    "        normalize_tfidf([word2vector[w] * word2word[w] for w in words if w in word2vector]\n",
    "                        or [np.zeros(dim)], [word2word[w] for w in words if w in word2vector or 0])\n",
    "        for words in content_separated\n",
    "    ])\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgQ0iZPCFy3l"
   },
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDFGgaKlFy3m"
   },
   "outputs": [],
   "source": [
    "def tuning_emb_vocab_size(vocabulary_names, content, content_seperated, interactions, uid_array, rids_array):\n",
    "    '''\n",
    "    Function tune size of vocabulary by trying out different vocabularies\n",
    "    '''\n",
    "    rmse_emb = []\n",
    "    \n",
    "    for vocabulary_name in vocabulary_names:\n",
    "        tfidf_matrix = get_tfidf_wordembedding_matrix(content_seperated, vocabulary_name)\n",
    "        similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        \n",
    "        predictions, actuals = make_all_predictions(len(interactions), similarity, content, interactions, uid_array, rids_array)\n",
    "        rmse = mean_squared_error(actuals, predictions)**0.5\n",
    "        rmse_emb.append(rmse)\n",
    "\n",
    "    return rmse_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPsE7lZ8Fy3o",
    "outputId": "8b26db6f-eff8-4b76-905f-4682adffa1da"
   },
   "outputs": [],
   "source": [
    "# Parameter space for the word embedding model, parameters represent size of embedding\n",
    "vocabulary_names = [\"glove-wiki-gigaword-50\", \"glove-wiki-gigaword-100\", \"glove-wiki-gigaword-200\", \"glove-wiki-gigaword-300\"]\n",
    "# Tokenized content required to tune the parameters\n",
    "content_tokenized = [content_item.split(\" \") for content_item in content_processed[\"content\"]]\n",
    "# Choose the size with the smallest RMSE on the dev set\n",
    "rmse_emb_tune = tuning_emb_vocab_size(vocabulary_names,\n",
    "                                      content_processed,\n",
    "                                      content_tokenized,\n",
    "                                      train_split,\n",
    "                                      train_split['user_id'].values,\n",
    "                                      train_split['recipe_id'].values)\n",
    "rmse_emb_min = min(rmse_emb_tune)\n",
    "rmse_emb_min_idx = rmse_emb_tune.index(rmse_emb_min)\n",
    "vocabulary_name_min = vocabulary_names[rmse_emb_min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "biQCh-IhFy3r",
    "outputId": "6870e28f-0e31-4b81-bc74-473cb7684dd9"
   },
   "outputs": [],
   "source": [
    "# Plot the RMSE for different sizes of word embeddings\n",
    "plt.plot(vocabulary_names, rmse_emb_tune)\n",
    "plt.xlabel('Vocabulary Name')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for different Word Embeddding models')\n",
    "plt.plot([vocabulary_name_min], [rmse_emb_min], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TkKm3lzFy3v"
   },
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1obLkeZFFy3v"
   },
   "outputs": [],
   "source": [
    "# Proximity matrix of the embedded model\n",
    "# tfidf_matrix = get_tfidf_wordembedding_matrix(content_tokenized, vocabulary_name_min)\n",
    "# similarity_matrix_embedded = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uJS-1lwFy3x"
   },
   "source": [
    "<a id='mixture'></a>\n",
    "### 3.2.2 Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IF2Z_1iJFy3y"
   },
   "source": [
    "#### Proximity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X41VzwpZFy3y"
   },
   "outputs": [],
   "source": [
    "def get_mix_sim_matrix(processed, lmbda, df_rfiltered):\n",
    "    '''\n",
    "    Function to compute proximity matrix for the mixture model\n",
    "    '''\n",
    "    cos_sim = get_cos_sim_matrix(processed, n_components_min, False)\n",
    "    df_sub = df_rfiltered[['recipe_id', 'n_steps', 'minutes', 'n_ingredients']]\n",
    "    df_processed = df_sub[df_sub['recipe_id'].isin(processed['recipe_id'])]\\\n",
    "                                                             .set_index('recipe_id')\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df_processed)\n",
    "    eucl_dis = euclidean_distances(X,X)\n",
    "    eucl_sim = 1 / np.exp(eucl_dis)\n",
    "    mixed_sim = np.add(cos_sim * lmbda, eucl_sim * (1-lmbda)) # assume equally weighted\n",
    "    return mixed_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0FsRlQkFy31"
   },
   "source": [
    "<a id='optimize-lambda'></a>\n",
    "#### Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JplzQs5SFy33"
   },
   "outputs": [],
   "source": [
    "def tuning_mix_param(lmbdas, content, df_rfiltered, interactions, uid_array, rids_array):\n",
    "    '''\n",
    "    Function computes the RMSE for different sizes of lambda (weighting parameter of the mixture model)\n",
    "    '''\n",
    "    rmse_mix = []\n",
    "    \n",
    "    for lmbda in lmbdas:\n",
    "        similarity = get_mix_sim_matrix(content, lmbda, df_rfiltered)\n",
    "        \n",
    "        predictions, actuals = make_all_predictions(len(interactions), \n",
    "                                                    similarity, \n",
    "                                                    content, interactions, \n",
    "                                                    uid_array, \n",
    "                                                    rids_array)\n",
    "        rmse = mean_squared_error(actuals, predictions)**0.5\n",
    "        rmse_mix.append(rmse)\n",
    "           \n",
    "    return rmse_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94lR7J5QFy36",
    "outputId": "1b212a58-0cd4-4469-99ec-a789b8ed7851"
   },
   "outputs": [],
   "source": [
    "# Set a suitable parameter range for lambda\n",
    "lmbdas = np.linspace(0,1,5)\n",
    "# Compute the RMSE for different lambdas (weighting paramtere)\n",
    "rmse_mix_tune = tuning_mix_param(lmbdas, \n",
    "                                 content_processed,\n",
    "                                 recipes_data,\n",
    "                                 train_split, \n",
    "                                 train_split['user_id'].values, \n",
    "                                 train_split['recipe_id'].values)\n",
    "rmse_mix_min = min(rmse_mix_tune)\n",
    "rmse_mix_min_idx = rmse_mix_tune.index(rmse_mix_min)\n",
    "lmbda_min = lmbdas[rmse_mix_min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdMTxVrzFy4A",
    "outputId": "47748f29-67d5-4399-a54d-2c7e63c2944c"
   },
   "outputs": [],
   "source": [
    "# Plot the RMSE for different sizes of lambda\n",
    "plt.plot(lmbdas, rmse_mix_tune)\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for different mixture models')\n",
    "plt.plot([lmbda_min], [rmse_mix_min], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQEPt8DqFy4E"
   },
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWoEM2pEFy4F"
   },
   "outputs": [],
   "source": [
    "# Proximity matrix of the mixture model\n",
    "# similarity_matrix_mixed = get_mix_sim_matrix(content_processed, lmbda_min, recipes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoX3ULIuFy4J"
   },
   "source": [
    "<a id='collab_filt'></a>\n",
    "### 3.3 Collaborative Filtering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVvkRhmtFy4L"
   },
   "outputs": [],
   "source": [
    "def create_pivot_table(pd_df):\n",
    "    '''\n",
    "    Function computes creates Pivot Matrix from dataframe\n",
    "    '''    \n",
    "    data = pd_df.values\n",
    "    rows, row_pos = np.unique(data[:, 0], return_inverse=True) #Retrieves sorted array of unqiue values of pd_df and the corresponding inverse in order to reconstruct the original array\n",
    "    cols, col_pos = np.unique(data[:, 1], return_inverse=True) #Retrieves sorted array of unqiue values of pd_df and the corresponding inverse in order to reconstruct the original array\n",
    "    pivot_matrix = np.zeros((len(rows), len(cols)), dtype=data.dtype)\n",
    "    pivot_matrix[row_pos, col_pos] = data[:, 2] #Matrix is column wise filled by the original positions of movieId and userId\n",
    "    return pivot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K0foutOFy4O"
   },
   "outputs": [],
   "source": [
    "def make_all_recommendations_cf(algo, trainset, neighbors, data_set, recipe_ids, k, return_est_ratings=False):\n",
    "    \"\"\"\n",
    "    Function returns the either a dict of users in which the values are lists of the recommended recipes or a list of ratings\n",
    "    for all recipes, depending on the parameter return_est_ratings\n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "    algo: Suprise Package Algorithm\n",
    "        A algorithm of the suprise package\n",
    "    trainset: Trainset\n",
    "        The suprise package trainset the algorithm was trained on\n",
    "    neighbors: int\n",
    "        The number of neighbors use for the algorithm in case it is KNN-inspired\n",
    "    data_set: pd.DataFrame\n",
    "        Our train split on which the models are trained\n",
    "    recipe_ids: list\n",
    "        All recipe ids we need predicitions for (usually the unique recipe ids from train and dev set)\n",
    "    k: integer\n",
    "        Number of recommendations to make, only considered if return_est_ratings is false\n",
    "    return_est_ratings: bool\n",
    "        Flag whether the top k list per user or all estimated recipe ratings per user should be returned\n",
    "    Returns:\n",
    "    --------\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Get the number of users and number of recipes\n",
    "    n_users = len(data_set.user_id.unique())\n",
    "    n_recipes = len(recipe_ids)\n",
    "    \n",
    "    # Get users we need a predicition for (assume that we have all users by now)\n",
    "    user_ids = data_set.user_id.unique()\n",
    "    user_ids.sort()\n",
    "    # Get all recipes we need a predicition for\n",
    "    all_recipes = np.array(recipe_ids, dtype = int)\n",
    "    all_recipes.sort()\n",
    "    # Get all recipes that are in the train set\n",
    "    recipes_train = data_set.recipe_id.unique()\n",
    "    # Get the baselines for users and items\n",
    "    bu, bi = algo.compute_baselines()\n",
    "\n",
    "    # Mapping between our user id and inner user id ==> suprise package uses inner id\n",
    "    # We always have all possible item ids due to our train-dev-test-split\n",
    "    our_user_ids = []\n",
    "    inner_user_ids = []\n",
    "    for uid in user_ids:\n",
    "        try:\n",
    "            # Collect the corresponding inner id\n",
    "            iid = trainset.to_inner_uid(uid)\n",
    "            inner_user_ids.append(iid)\n",
    "        except:\n",
    "            pass\n",
    "        # Collect the corresponding our id\n",
    "        our_user_ids.append(uid)\n",
    "    mapping_uid = pd.DataFrame({'inner_user_id': inner_user_ids, 'bu': bu}, index=our_user_ids)\n",
    "    mapping_uid.sort_index(inplace = True)\n",
    "    mapping_uid.index.name = 'user_id'\n",
    "    mapping_uid.reset_index(inplace = True)\n",
    "    \n",
    "    # Check whether inner user ids are sorted the same way our user ids are sorted\n",
    "    if mapping_uid.inner_user_id.is_monotonic is False:\n",
    "        raise Exception(\"Train split is not sorted by user id\")\n",
    "    \n",
    "    # Important: Our user ids are sorted by the inner user ids (ascending) ==> can rely on implicit sorting\n",
    "\n",
    "    # Mapping between our recipe id and inner recipe id ==> suprise package uses inner id\n",
    "    # Get the item ids that exist in the data ses.\n",
    "    our_recipe_ids = []\n",
    "    inner_recipe_ids = []\n",
    "    for rid in recipes_train:\n",
    "        try:\n",
    "            # Collect the corresponding inner id\n",
    "            iid = trainset.to_inner_iid(rid)\n",
    "            inner_recipe_ids.append(iid)\n",
    "        except:\n",
    "            pass\n",
    "        # Collect the corresponding our id\n",
    "        our_recipe_ids.append(rid)\n",
    "    # Build the data frame with the mapping\n",
    "    mapping_iid = pd.DataFrame({'inner_recipe_id': inner_recipe_ids, 'bi': bi}, index=our_recipe_ids)\n",
    "    \n",
    "    # Add missing items to the mapping iid dataframe, that were lost because of our train test split\n",
    "    all_recipe_ids_index = pd.Index(all_recipes)\n",
    "    mapping_iid_all = mapping_iid.reindex(all_recipe_ids_index)\n",
    "    mapping_iid_all.sort_index(inplace = True)\n",
    "    # Fill missing values for baseline\n",
    "    mapping_iid_all.bi.fillna(0, inplace = True)\n",
    "    # Reset index to get recipe ids back in columns\n",
    "    mapping_iid.index.name = 'recipe_id'\n",
    "    mapping_iid.reset_index(inplace = True)\n",
    "    \n",
    "    ### Important: Our recipe ids are not sorted by the inner recipe ids ==> always check for match, do not rely on implicit sorting\n",
    "\n",
    "    # Creating an array of shape n_users x n_recipes with all the recipe baselines\n",
    "    ### Sorted ascending by our user id\n",
    "    bi_matrix = np.tile(mapping_iid_all.bi.to_numpy(), (n_users, 1))\n",
    "    # Creating an array of shape n_users x n_recipes with all the user baselines\n",
    "    ### Sorted ascending by our recipe id\n",
    "    bu_matrix = np.tile(mapping_uid.bu.to_numpy(), (n_recipes,1)).T\n",
    "    \n",
    "    # Build matrix of n_users x n_recipes with the already rated items to mask the estimation matrix \n",
    "    # ==> we only want to recommend recipes that are not already rated\n",
    "    # We need to make sure that all recipes are in the in our dataframe before we build the pivot, \n",
    "    # thus we will add dummy rows for one user that contain all missing recipes with rating 0\n",
    "    missing_recipe_ids = all_recipes[np.isin(all_recipes, recipes_train, invert=True)]\n",
    "    dummy_interactions = pd.DataFrame({'user_id': np.repeat(user_ids[0], len(missing_recipe_ids)), \n",
    "                                       'recipe_id': missing_recipe_ids, \n",
    "                                       'rating': np.repeat(0, len(missing_recipe_ids))}, columns=['user_id', 'recipe_id', 'rating'])\n",
    "    data_set_extend = data_set[['user_id', 'recipe_id', 'rating']].append(dummy_interactions)\n",
    "    \n",
    "    # Pivot ensures ascending sorting of the user ids in rows and of the item ids in columns\n",
    "    user_recipe_ratings_pivot = create_pivot_table(data_set_extend)\n",
    "    \n",
    "    # Estimate the ratings depending on model\n",
    "    if type(algo) is BaselineOnly:\n",
    "        print('Estimate BaselineOnly')\n",
    "        # Compute the estimation\n",
    "        est = bi_matrix + bu_matrix + trainset.global_mean\n",
    "        \n",
    "    elif type(algo) is KNNBaseline:\n",
    "        print('Estimate KNNBaseline')\n",
    "        # Get the user item baseline matrix\n",
    "        bui_matrix = bi_matrix + bu_matrix + trainset.global_mean\n",
    "        # Get the user item matrix masked by the user item pivot of actual ratings\n",
    "        bui_matrix_masked = np.where(user_recipe_ratings_pivot == 0, 0, bui_matrix)\n",
    "        \n",
    "        if algo.sim_options['user_based']:\n",
    "            ## User Based\n",
    "            # Get the similarity matrix\n",
    "            similarity = algo.compute_similarities()\n",
    "            # Maske the similarity matrix by k neighbors\n",
    "            for i in range(0, len(similarity)):\n",
    "                # Get the sorted unique values and the indices of the former array\n",
    "                values, indices = np.unique(similarity[i], return_inverse=True)\n",
    "                # Set everything to zero except from the top k+1 neighbors (+1 for the similarity to the own user which is 1)\n",
    "                values[:-(k+1)] = 0\n",
    "                # Get the sorting of former array back and replace in similarity matrix\n",
    "                similarity[i] = values[indices]\n",
    "\n",
    "            # Compute the estimation\n",
    "            ratings_diff = np.matmul((user_recipe_ratings_pivot - bui_matrix_masked).T, similarity)\n",
    "            # Normalize by the sum of weights-1 (as the own user is in the axis with a similarity of 1)\n",
    "            # Add the user-item baseline ==> Set the est to zero in case there are not similar users\n",
    "            est = (ratings_diff / np.array([np.abs(similarity).sum(axis=1) - 1])).T\n",
    "            # Workaround for division by 0\n",
    "            est[est == np.inf] = 0\n",
    "            est[est == np.NINF] = 0\n",
    "            # Add user item baseline matrix\n",
    "            est += bui_matrix\n",
    "        else:\n",
    "            raise Exception(\"Item based mass prediction not implemented\")\n",
    "    elif type(algo) is SVD:\n",
    "            # Get item x factor matrix\n",
    "            qi = algo.qi\n",
    "            # Get user x factor matrix\n",
    "            pu = algo.pu\n",
    "            # Get user x items baseline matrix\n",
    "            bui_matrix = bi_matrix + bu_matrix + trainset.global_mean\n",
    "            # Get user x item factor matrix\n",
    "            factor_matrix = np.matmul(pu, qi.T)\n",
    "            est = np.zeros(bui_matrix.shape)\n",
    "            # Cop whole bui matrix to get predictions even for user item combinations we did not see in training \n",
    "            est = bui_matrix\n",
    "            # Add the factor matrix to all combinations we did see in training\n",
    "            est[:,np.isin(all_recipes, mapping_iid.recipe_id.to_numpy())] = bui_matrix[:,np.isin(all_recipes, mapping_iid.recipe_id.to_numpy())] + factor_matrix \n",
    "    ###\n",
    "    \n",
    "    # Setting the estimation of the already rated recipes to zero, so that those end up at the end during sorting\n",
    "    est_masked = np.where(user_recipe_ratings_pivot == 0, est, 0)\n",
    "\n",
    "    # Build results\n",
    "    rec_dict = {}\n",
    "\n",
    "    # Get the top k recipes per user and put them in a dict\n",
    "    for i in range(0, n_users):\n",
    "        uid = data_set.user_id.unique()[i]\n",
    "        if return_est_ratings:\n",
    "            rec_dict[uid] = est_masked[i]\n",
    "        else:\n",
    "            ratings_per_user = pd.DataFrame({'recipe_id': all_recipes, 'rating': est_masked[i]})\n",
    "            ratings_per_user.sort_values('rating', ascending=False, inplace=True)\n",
    "            top_k_per_user = ratings_per_user.recipe_id.iloc[:k].to_numpy()\n",
    "            rec_dict[uid] = top_k_per_user\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'Runtime prediction of all ratings: {round(end-start, 2)}')\n",
    "    \n",
    "    return rec_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZ2JGYMGFy4U"
   },
   "source": [
    "#### Preselection of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkclF9ZKFy4U",
    "outputId": "6c964cd9-5cb1-42ff-a888-7fe5d6b46731",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare Results of default algorihms on the problem to identify most promising once\n",
    "\n",
    "algos = {\n",
    "    'NormalPredictor': NormalPredictor, \n",
    "    'BaselineOnly': BaselineOnly, \n",
    "    'KNNBasic': KNNBasic, \n",
    "    'KNNWithMeans': KNNWithMeans,\n",
    "    'KNNWithZScore': KNNWithZScore,\n",
    "    'KNNBaseline': KNNBaseline,\n",
    "    'SVD': SVD,\n",
    "}\n",
    "\n",
    "# Build cross validation split\n",
    "cv_cf = ShuffleSplit(n_splits = 10, test_size = 0.3, random_state = 42, shuffle = True)\n",
    "\n",
    "# Create dataset objects from the train-test-split which is required for suprise package \n",
    "trainset = Dataset.load_from_df(train_split[['user_id','recipe_id', 'rating']], Reader())\n",
    "# Results dict\n",
    "results_algos_cf = {}\n",
    "\n",
    "# Evaluate each algorithm with default settings\n",
    "for name, algo in algos.items():\n",
    "    cs = cross_validate(algo(), trainset, cv = cv_cf, n_jobs = -1)\n",
    "    results_algos_cf.update({name: [np.mean(cs.get('test_rmse'))]})\n",
    "\n",
    "results_algos_cf_df = pd.DataFrame.from_dict(results_algos_cf, orient = 'index', columns = ['DevRMSE'])\n",
    "results_algos_cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzbuOZuJFy4Y"
   },
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJ_FjpGiFy4Y",
    "outputId": "f75a0adc-3a5e-4b93-8e69-be8bdb867f3f"
   },
   "outputs": [],
   "source": [
    "# Evaluate each algorithm with tuned parameters\n",
    "algos = { \n",
    "    'BaselineOnly': BaselineOnly,\n",
    "    'KNNWithMeans': KNNWithMeans,\n",
    "    'SVD': SVD,\n",
    "}\n",
    "\n",
    "# Store the fitted algorithms for later use\n",
    "fitted_algos = {}\n",
    "\n",
    "# List to store the final results for the estimators in\n",
    "benchmark = []\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"KNNWithMeans\" : {\n",
    "        \"k\" : [10, 20, 40],\n",
    "        \"sim_options\" : {\n",
    "            'user_based': [True, False],\n",
    "            \"name\" : ['cosine', 'msd', 'pearson']\n",
    "        },\n",
    "    },     \n",
    "    \"SVD\" : {\n",
    "        \"n_factors\" : [5, 10, 100],\n",
    "        \"lr_all\": [0.0025, 0.005, 0.01],\n",
    "        \"reg_all\": [0.01, 0.02, 0.04]\n",
    "    },\n",
    "    \"BaselineOnly\" : {}\n",
    "}\n",
    "\n",
    "for name, algo in algos.items():\n",
    "    print(name)\n",
    "    gs = GridSearchCV(algo, param_grid.get(name), measures=['rmse'], cv=cv_cf, refit=False)\n",
    "    \n",
    "    start_gs = time.time()\n",
    "    # Fit the estimator\n",
    "    gs.fit(trainset)\n",
    "    end_gs = time.time()\n",
    "    print(f'GridSearch: {round(end_gs-start_gs, 2)}s')\n",
    "    \n",
    "    # Get the best results per estimator\n",
    "    best_index_rmse = gs.best_index['rmse']\n",
    "    best_params_rmse = gs.best_params['rmse']\n",
    "    rmse = gs.cv_results['mean_test_rmse'][best_index_rmse]\n",
    "    \n",
    "    # Store the results\n",
    "    benchmark.append([name, rmse, best_params_rmse])\n",
    "\n",
    "# Display the results\n",
    "results_cf_df = pd.DataFrame(benchmark, columns = [\"Algorithm\", \"DevRMSE\", \"BestParams\"])\n",
    "results_cf_df.set_index([\"Algorithm\"], inplace = True)\n",
    "results_cf_df.sort_values('DevRMSE', inplace = True)\n",
    "results_cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdrWhUb1Fy4b"
   },
   "source": [
    "<a id='evaluation'></a>\n",
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SW8tDRahFy4c"
   },
   "source": [
    "<a id='single_algos'></a>\n",
    "### 4.1 Single Algorithms Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19544,
     "status": "ok",
     "timestamp": 1590066201747,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "2iVZ9X6VFy4c",
    "outputId": "c489450b-2d4f-46bc-bbcb-9ae12550b64d"
   },
   "outputs": [],
   "source": [
    "# Create a train dev split the same way we created the train test split (only single possible) ==> Fixed N\n",
    "# Required as we used different splits for hyperparameter tuning for content based and collaborative filtering\n",
    "# To compare the classification metrics, we have to use the same split\n",
    "fixed_n_dev = 3\n",
    "train_split, dev_split = get_fixedN_test_train_split(train_split, fixed_n_dev)\n",
    "# Validate quality of train-dev split\n",
    "print(f'Test size: {round((len(dev_split) / len(train_split)) * 100, 2)}% \\n')\n",
    "print('Rating distribution in test set \\n')\n",
    "get_rating_dist(dev_split.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbAOWGZlFy4g",
    "outputId": "cf63efaf-9b96-4810-c811-2764489885db"
   },
   "outputs": [],
   "source": [
    "# Define k for the number of Top recipes\n",
    "k = 10\n",
    "\n",
    "# Store the resuls of the different models\n",
    "benchmark = []\n",
    "\n",
    "# Get all recipes for which we need a recommendation\n",
    "recipe_ids = list((set().union(train_split.recipe_id.unique(), dev_split.recipe_id.unique())))\n",
    "\n",
    "# Filter only the recipes for which we need a recommendation\n",
    "filtered_content_processed = content_processed.loc[content_processed['recipe_id'].isin(recipe_ids)]\n",
    "\n",
    "# Recalculate tfidf-matrix for embeddings\n",
    "content_tokenized = [content_item.split(\" \") for content_item in filtered_content_processed[\"content\"]]\n",
    "tfidf_matrix = get_tfidf_wordembedding_matrix(content_tokenized, vocabulary_name_min)\n",
    "\n",
    "# Calculate scores for content based models\n",
    "content_based_models = {\n",
    "    'Cosine SVD Model': get_cos_sim_matrix(filtered_content_processed, n_components_min, use_svd=True),\n",
    "    'Cosine Plain Model': get_cos_sim_matrix(filtered_content_processed, n_components_min, use_svd=False),\n",
    "    'Embedded Cosine Model': cosine_similarity(tfidf_matrix, tfidf_matrix),\n",
    "    'Mixture Model': get_mix_sim_matrix(filtered_content_processed, lmbda_min, recipes_data)\n",
    "}\n",
    "\n",
    "for m_name, model in content_based_models.items():\n",
    "    rec_dict = make_all_recommendations(dev_split.user_id.unique(), model, content_processed, train_split, k)\n",
    "\n",
    "    # Compute metrics\n",
    "    # Catalog coverage\n",
    "    cat_cov = catalog_coverage(rec_dict, n_recipes)\n",
    "    # Get hits per user (required for other metrics)\n",
    "    n_hits_per_user = get_hits(rec_dict, dev_split)\n",
    "    # Average precision\n",
    "    precision = get_avg_precision(n_hits_per_user, k)\n",
    "    # Average recall\n",
    "    recall = get_avg_recall(n_hits_per_user, fixed_n_dev)\n",
    "    # f1-score\n",
    "    f_one = get_f_one(precision, recall)\n",
    "    # Hitrate\n",
    "    hitr = hitrate(n_hits_per_user)\n",
    "    \n",
    "    benchmark.append([m_name, cat_cov, precision, recall, f_one ,hitr])\n",
    "\n",
    "\n",
    "# Prepare train and dev set for algos\n",
    "# Create dataset objects from the train-test-split which is required for suprise package\n",
    "trainset = Dataset.load_from_df(train_split[['user_id','recipe_id', 'rating']], Reader()).build_full_trainset()\n",
    "devset = Dataset.load_from_df(dev_split[['user_id','recipe_id', 'rating']], Reader()).build_full_trainset().build_testset()\n",
    "\n",
    "# Hard coded backup of best parameters\n",
    "# algos = { \n",
    "#     'BaselineOnly': BaselineOnly(),\n",
    "#     'KNNBaseline': KNNBaseline(k=10, sim_options={'user_based': True, 'name': 'pearson'}),\n",
    "#     'SVD': SVD(n_factors=10, lr_all=0.005, reg_all=0.04),\n",
    "# }\n",
    "\n",
    "for name, algo in algos.items():\n",
    "    print(name)\n",
    "    algo = algo(**results_cf_df.loc[name, 'BestParams'])\n",
    "    print(\"Train Started\")\n",
    "    # Fit the algorithm on the train set\n",
    "    algo.fit(trainset)\n",
    "    print(\"Prediction Started\")\n",
    "    start = time.time()\n",
    "    # Get predictions on dev set\n",
    "    predictions = algo.test(devset)\n",
    "    end = time.time()\n",
    "    print(f'Runtime prediction of ratings in dev set: {round(end-start,2)}')\n",
    "    # Get Top k recipes\n",
    "    rec_dict = make_all_recommendations_cf(algo, trainset, 10, train_split, recipe_ids, k)\n",
    "    # Catalog coverage\n",
    "    cat_cov = catalog_coverage(rec_dict, n_recipes)\n",
    "    # Get hits per user (required for other metrics)\n",
    "    n_hits_per_user = get_hits(rec_dict, dev_split)\n",
    "    # Average precision\n",
    "    precision = get_avg_precision(n_hits_per_user, k)\n",
    "    # Average recall\n",
    "    recall = get_avg_recall(n_hits_per_user, fixed_n_dev)\n",
    "    # f1-score\n",
    "    f_one = get_f_one(precision, recall)\n",
    "    # Hitrate\n",
    "    hitr = hitrate(n_hits_per_user)\n",
    "    \n",
    "    benchmark.append([name, cat_cov, precision, recall, f_one, hitr])\n",
    "    \n",
    "benchmark_df = pd.DataFrame(benchmark, columns = ['Model', 'Catalog Coverage', 'Precision', 'Recall', 'f1' ,'Hitrate'])\n",
    "benchmark_df.set_index('Model', inplace = True)\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJrkpMBPFy4i"
   },
   "source": [
    "<a id='hybrid_model'></a>\n",
    "### 3.3 Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aSs7xBOFy4j"
   },
   "source": [
    "#### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRmUNwCwFy4k"
   },
   "outputs": [],
   "source": [
    "def get_user_preference_cb_for_hybrid(user_id, similarity, content, interactions_data):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    ----------\n",
    "    prediction_df:\n",
    "        DataFrame in with columns ['recipe_id', 'prediction', 'has_rated'] for 1 user\n",
    "    \"\"\"\n",
    "    #prepare similarity dataframe\n",
    "    sim = pd.DataFrame(similarity, index = content['recipe_id'].values, columns = content['recipe_id'].values)\n",
    "    #get already rated recipes of user\n",
    "    rated_recipes = interactions_data.loc[interactions_data['user_id'] == user_id, 'recipe_id'].values\n",
    "    #get similarities of ALL recipes w/ already rated recipes of user\n",
    "    sim_rated_all = sim.loc[rated_recipes, :]\n",
    "    #get ratings of already rated recipes\n",
    "    ratings = get_reshaped_ratings(user_id, interactions_data)\n",
    "    \n",
    "    #compute weighted similarities between all recipes and already rated recipes\n",
    "    weighted_sim = np.dot(ratings,sim_rated_all)\n",
    "    #compute normalization constant\n",
    "    norm_const = np.array(np.abs(sim_rated_all).sum(axis=0))\n",
    "    #return sorted predictions\n",
    "    pref_predictions = weighted_sim / norm_const\n",
    "    \n",
    "    flat_predictions = [item for sublist in pref_predictions for item in sublist]\n",
    "    #return df with recipe id also\n",
    "    prediction_df = pd.DataFrame(flat_predictions, index=content['recipe_id'].values, columns =['prediction'])\n",
    "    #indicate the already tried recipes\n",
    "    prediction_df['has_rated'] = prediction_df.index.isin(rated_recipes)\n",
    "    #order predictions\n",
    "\n",
    "    prediction_df['prediction'] = prediction_df.apply(lambda x: 0 if x.has_rated == True else x.prediction, axis=1)\n",
    "    \n",
    "    return prediction_df.prediction.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsLgn-VrJgxc"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "fixed_n_test = 3\n",
    "train_split, test_split = get_fixedN_test_train_split(interactions, fixed_n_test)\n",
    "\n",
    "fixed_n_dev = 3\n",
    "train_split, dev_split = get_fixedN_test_train_split(train_split, fixed_n_dev)\n",
    "\n",
    "recipe_ids = list((set().union(train_split.recipe_id.unique(), dev_split.recipe_id.unique())))\n",
    "trainset = Dataset.load_from_df(train_split[['user_id', 'recipe_id', 'rating']], Reader()).build_full_trainset()\n",
    "devset = Dataset.load_from_df(dev_split[['user_id', 'recipe_id', 'rating']], Reader()).build_full_trainset().build_testset()\n",
    "\n",
    "# Filter only the recipes for which we need a recommendation\n",
    "filtered_content_processed = content_processed.loc[content_processed['recipe_id'].isin(recipe_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582,
     "status": "error",
     "timestamp": 1590067499021,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "amzi-w0GYHuG",
    "outputId": "2810a6f6-5be1-42d0-f0e6-06f79a47b025"
   },
   "outputs": [],
   "source": [
    "# Specify models to be combined in a hybrid model\n",
    "cf_algos = {\n",
    "    'KNNBaseline': KNNBaseline\n",
    "}\n",
    "cb_algos = {\n",
    "    'Cosine SVD Model': get_cos_sim_matrix(filtered_content_processed, 200, use_svd = True),\n",
    "    'Cosine Plain Model': get_cos_sim_matrix(filtered_content_processed, 200, use_svd = False)\n",
    "}\n",
    "# Specify model weighting\n",
    "algo_weighting = {\n",
    "    'KNNBaseline': 1,\n",
    "    'Cosine SVD Model': 0.5,\n",
    "    'Cosine Plain Model': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 681,
     "status": "error",
     "timestamp": 1590067493739,
     "user": {
      "displayName": "Jannik Brinkmann",
      "photoUrl": "https://lh5.googleusercontent.com/-IEFZGn3O0N0/AAAAAAAAAAI/AAAAAAAAALg/IH5WMl22mmI/s64/photo.jpg",
      "userId": "13523775422752284681"
     },
     "user_tz": -120
    },
    "id": "523zl2XRHrF9",
    "outputId": "7850bb2b-c7af-4d8e-acca-847a8b414c3b"
   },
   "outputs": [],
   "source": [
    "def make_all_recommendations_hm(cf_algos, cb_algos, algo_weighting, trainset, train_split, recipe_ids, filtered_content_processed, k):\n",
    "\n",
    "    # Initialize empty dictionary for collaborative-filtering predictions\n",
    "    est_cf = {}    \n",
    "    # Make predictions for each collaborative-filting algorithm\n",
    "    for name, algo in cf_algos.items():\n",
    "        # Choose best parameters from hyperparameter tuning\n",
    "        algo = algo()\n",
    "        # Fit the algorithm on the train set\n",
    "        algo.fit(trainset)\n",
    "        # Get predictions on dev set\n",
    "        predictions = make_all_recommendations_cf(algo, trainset, 10, train_split, recipe_ids, k, return_est_ratings = True)\n",
    "        # Weight predictions\n",
    "        weight = algo_weighting.get(name)\n",
    "        predictions.update((key, np.dot(weight, value)) for key, value in predictions.items())\n",
    "        # Add weighted predictions\n",
    "        est_cf = {key: est_cf.get(key, 0) + predictions.get(key, 0)\n",
    "                  for key in set(predictions)}\n",
    "        \n",
    "    # Initialize empty dictionary for top-k predictions\n",
    "    results = {}\n",
    "    # Make predictions for content-based algorithms    \n",
    "    for user, est_cf in est_cf.items():\n",
    "        # Get weighting of the collaborative-filtering algorithms for the user\n",
    "        est_all = est_cf\n",
    "        # Make predictions for each content-based algorithm\n",
    "        for name, algo in cb_algos.items():\n",
    "            est_cb = get_user_preference_cb_for_hybrid(user, cb_algos.get(name), filtered_content_processed, interactions)\n",
    "            weight = algo_weighting.get(name)\n",
    "            est_all = est_all + np.dot(weight, est_cb)\n",
    "        # Get the weighted average prediction of all algorithms\n",
    "        est_all = est_all / sum(algo_weighting.values())\n",
    "        # Get top-k predictions\n",
    "        ratings_per_user = pd.DataFrame({'recipe_id': recipe_ids, 'rating': est_all})\n",
    "        ratings_per_user.sort_values('rating', ascending = False, inplace = True)\n",
    "        top_k_per_user = ratings_per_user.recipe_id.iloc[:k].to_numpy()\n",
    "        results[user] = top_k_per_user\n",
    "    return results\n",
    "\n",
    "recs = make_all_recommendations_hm(cf_algos, cb_algos, algo_weighting, \n",
    "                                   trainset, train_split, \n",
    "                                   recipe_ids, filtered_content_processed, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_values(cb_algos, trainset, train_split, recipe_ids, filtered_content_processed, k):\n",
    "    # Choose best parameters from hyperparameter tuning\n",
    "    knn = KNNBaseline()\n",
    "    # Fit the algorithm on the train set\n",
    "    knn.fit(trainset)\n",
    "    # Get predictions on dev set\n",
    "    est_cf_knn = make_all_recommendations_cf(knn, trainset, 10, train_split, recipe_ids, k, return_est_ratings = True)\n",
    "        \n",
    "    # Initialize empty dictionary for predicted values\n",
    "    est_cb_plain = {} \n",
    "    # Make predictions for Cosine Plain\n",
    "    for user, est_cf in est_cf_knn.items():\n",
    "        est_cb_plain[user] = get_user_preference_cb_for_hybrid(user, cb_algos.get(\"Cosine Plain Model\"), filtered_content_processed, interactions)\n",
    "\n",
    "    # Initialize empty dictionary for predicted values\n",
    "    est_cb_svd = {} \n",
    "    # Make predictions for Cosine Plain\n",
    "    for user, est_cf in est_cf_knn.items():\n",
    "        est_cb_svd[user] = get_user_preference_cb_for_hybrid(user, cb_algos.get(\"Cosine SVD Model\"), filtered_content_processed, interactions)\n",
    "    \n",
    "    return est_cf_knn, est_cb_plain, est_cb_svd\n",
    "\n",
    "est_cf_knn, est_cb_plain, est_cb_svd = get_predicted_values(cb_algos, trainset, train_split, recipe_ids, filtered_content_processed, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models to be combined in a hybrid model\n",
    "cf_algos = {\n",
    "    'KNNBaseline': KNNBaseline\n",
    "}\n",
    "cb_algos = {\n",
    "    'Cosine SVD Model': get_cos_sim_matrix(filtered_content_processed, 200, use_svd = True),\n",
    "    'Cosine Plain Model': get_cos_sim_matrix(filtered_content_processed, 200, use_svd = False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_model_weighting(est_cf_knn, est_cb_plain, est_cb_svd, recipe_ids, n_recipes, dev_split, k, fixed_n_dev):\n",
    "    # Generate a number of potential weightings that sum to 1\n",
    "    range = np.linspace(0, 1, 5)\n",
    "    parameter_space = pd.DataFrame(list(itertools.product(range, range, range)))\n",
    "    parameter_space = parameter_space.loc[(parameter_space.sum(axis=1) == 1), ]\n",
    "    \n",
    "    # Initialize empty list to store performance of different weighting\n",
    "    benchmark = []\n",
    "    \n",
    "    # Evaluate each weighting option in the parameter space\n",
    "    for index, weights in parameter_space.iterrows():       \n",
    "        # Sum up the weightings\n",
    "        predictions = {key: weights[0] * est_cf_knn.get(key, 0) + weights[1] * est_cb_plain.get(key, 0) + weights[2] * est_cb_svd.get(key, 0)\n",
    "                       for key in set(est_cf_knn)}\n",
    "        \n",
    "        # Initialize empty dictionary for top-k predictions\n",
    "        recs = {}\n",
    "        \n",
    "        # Specify top-k recommendations per user\n",
    "        for user, estimates in predictions.items():\n",
    "            ratings_per_user = pd.DataFrame({'recipe_id': recipe_ids, 'rating': estimates})\n",
    "            ratings_per_user.sort_values('rating', ascending = False, inplace = True)\n",
    "            top_k_per_user = ratings_per_user.recipe_id.iloc[:k].to_numpy()\n",
    "            recs[user] = top_k_per_user\n",
    "        \n",
    "        # Calculate evaluation measures\n",
    "        cat_cov = catalog_coverage(recs, n_recipes) # Catalogue coverage\n",
    "        n_hits_per_user = get_hits(recs, dev_split) # Hits per user\n",
    "        precision = get_avg_precision(n_hits_per_user, k) # Average precision\n",
    "        recall = get_avg_recall(n_hits_per_user, fixed_n_dev) # Recall\n",
    "        f_one = get_f_one(precision, recall) # F1-Score\n",
    "        hitr = hitrate(n_hits_per_user) # Hitrate\n",
    "        \n",
    "        # Append results to data frame\n",
    "        print(weights, cat_cov, precision, recall, f_one, hitr)\n",
    "        benchmark.append([weights, cat_cov, precision, recall, f_one, hitr])        \n",
    "    return benchmark\n",
    "\n",
    "benchmark = optim_model_weighting(est_cf_knn, est_cb_plain, est_cb_svd, recipe_ids, n_recipes, dev_split, k, fixed_n_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform benchmark list to dataframe \n",
    "benchmark_df = pd.DataFrame(benchmark, columns=['Weighting', 'Catalog Coverage', 'Precision', 'Recall', 'f1' ,'Hitrate'])\n",
    "benchmark_df.set_index('Weighting', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert date in string format of yyyy-mm-dd\n",
    "def show_me_something_new(user_id, dateStr, recipes, k):\n",
    "    \"\"\"\n",
    "     Returns\n",
    "    ----------\n",
    "    topk_recipes:\n",
    "        array of top k recipe ids\n",
    "    name:\n",
    "        array with top k recipe names\n",
    "    minutes:\n",
    "        array with top k recipe duration in minutes\n",
    "    submitted:\n",
    "        array with top k recipe date of submission\n",
    "    description:\n",
    "        array with top k recipe descriptions\n",
    "    recipeurl:\n",
    "        array with top k urls to full recipe\n",
    "    imageurl:\n",
    "        array with top k urls to recipe image\n",
    "    \"\"\"\n",
    "    # Transform date string in datetime format\n",
    "    date = datetime.strptime(dateStr, '%Y-%m-%d')\n",
    "    \n",
    "    # Get all recipes that are no older than two weeks from today\n",
    "    twoweeksago = date - timedelta(days=14)\n",
    "    newRecipes = recipes[recipes['submitted'] > twoweeksago]\n",
    "    \n",
    "    # Order all recipes descending by avg_rating and num_interactions\n",
    "    ordered_recipes = newRecipes.sort_values(by=['avg_rating', 'num_interactions'], ascending=False)\n",
    "    #get top k recipe_id array of most rated and best rated recipes\n",
    "    topk_recipes = ordered_recipes.index[:k].values\n",
    "\n",
    "    imageurls = []\n",
    "    recipeurls = []\n",
    "    for entry in topk_recipes:\n",
    "        recipeurls.append(\"https://www.food.com/recipe/\" + str(entry))\n",
    "        imageurls.append(get_image_source_url(entry))\n",
    "\n",
    "    info = recipes.loc[topk_recipes]\n",
    "    info = info[['name', 'minutes', 'submitted', 'description']]\n",
    "    info['recipeurl'] = recipeurls\n",
    "    info['imageurl'] = imageurls\n",
    "\n",
    "    for index, row in info.iterrows():\n",
    "        info.at[index, 'recipeurl'] = '<a href=\"'+ row['recipeurl'] + '\">'+row['recipeurl'] +'</a>'\n",
    "        info.at[index, 'imageurl'] = '<a href=\"'+ row['imageurl'] + '\"> Image of recipe '+str(index)+'</a>'\n",
    "    info = HTML(info.to_html(escape=False))\n",
    "    return(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6FVZYk4Fy49"
   },
   "source": [
    "<a id='final_scores'></a>\n",
    "### 4.2 Final Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all recipes for which we need a recommendation\n",
    "recipe_ids = list((set().union(train_split.recipe_id.unique(), dev_split.recipe_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbCEEGbIFy4-"
   },
   "outputs": [],
   "source": [
    "# Test set for suprise package\n",
    "testset = Dataset.load_from_df(test_split[['user_id','recipe_id', 'rating']], Reader()).build_full_trainset().build_testset()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e_qL82TJFy3J",
    "7K7TMjECFy3i",
    "tgQ0iZPCFy3l",
    "5TkKm3lzFy3v",
    "4uJS-1lwFy3x",
    "IF2Z_1iJFy3y",
    "D0FsRlQkFy31",
    "wQEPt8DqFy4E",
    "yTpN_MUQFy41",
    "0EzWHdaTFy45",
    "M6FVZYk4Fy49"
   ],
   "name": "Content Mining Overall-BenesMerge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
